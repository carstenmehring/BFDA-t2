---
title: "Bayes factor design analysis"
author: "Felix Schönbrodt"
date: "4 August 2016"
output: html_document
---

```{r setup, include=FALSE}
	knitr::opts_chunk$set(cache=TRUE)
	# load all functions
	devtools::load_all("../package")
```
## The general workflow ##

1. Simulate many hypothetical studies, both under H1 and under H0, using the function `BFDA.sim`
2. Analyze the simulated studies

To summarize, the general workflow is (here shown without parameters; these are discussed later):

```{r eval=FALSE}
sim.H1 <- BFDA.sim(expected.ES = 0.5, ...)
sim.H0 <- BFDA.sim(expected.ES = 0, ...)

BFDA.analyze(H1)
BFDA.analyze(H0)

plot(sim.H1)
plot(sim.H0)

SSD(sim.H1)
SSD(sim.H0)
```


## Simulating hypothetical studies for a prospective design analysis

As we do not know in advance whether H1 or H0 provide a better predictive performance of real world data, we want to evaluate the performance of a design under *both* hypotheses. Hence, we have to simulate a "H1 world" and a "H0 world":

```{r sim, cache=TRUE, results='hide'}
sim.H1 <- BFDA.sim(expected.ES=0.5, type="t.between", n.min=20, n.max=300, alternative="directional", boundary=Inf, B=100, verbose=TRUE, cores=2)

sim.H0 <- BFDA.sim(expected.ES=0, type="t.between", n.min=20, n.max=300, alternative="directional", boundary=Inf, B=100, verbose=TRUE, cores=2)
```

Let's go through the most important parameters (for a full list of options, see `?BFDA.sim`):

- `expected.ES`: The assumed effect size (ES). In classical power analysis, this is a fixed number. Here, you can also provide a vector, which quantifies the uncertainty about the true ES. For example: `expected.ES=rnorm(100000, 0.5, 0.1)`. If a vector is provided, a new ES is drawn from this vector for each simulated study. The **metric** for `expected.ES` depends on the type of design (see net bullet point):
	- `type = "t.between` or `type = "t.paired`": expected.ES has to be provided as Cohen's *d*
	- `type = "correlation"`: expected.ES has to be provided as correlation
- `type`: Type of design. Currently, 3 designs are implemented: A between-group t-test ("t.between"), a paired t-test ("t.paired"), and correlations ("correlation")
- `n.min` and `n.max`: The initial sample size and the maximum sample size that is tested in the sequential procedure.
- `alternative`: Either "directional" for directed hypotheses (default) or "undirected" for two-sided tests
- `B`: Number of simulated studies. Aim for B >= 10,000 for stable results (in this document we use B=1000 to save some time).
- `cores`: Multicore support. Add as many cores as you have to speed up computations.


The simulations of the "H1 world" and a "H0 world" should have the same parameters (alternative, rscale, boundary, n.min) except the `expected.ES` (in the actual data analysis, we will apply the same test to the data set, regardless whether data came from H0 or H1 (what we don't know anyway.)).

The BFDA uses the BayesFactor package to compute between and paired t-test. By default, it uses an rscale of sqrt(2)/2 for the JZS prior on effect sizes under H1. For correlations, it uses the code provided by Wagenmakers, E. J., Verhagen, J., & Ly, A. (2016). How to quantify the evidence for the absence of a correlation. Behavior Research Methods, 1–14. http://doi.org/10.3758/s13428-015-0593-0 (see https://osf.io/cabmf/).

By default, a full sequential design without evidential stopping threshold is simulated. That means, ...
That allows to extract ...

## Analyze the simulations ##

Next, we can retrieve summary statistics from our simulations. For these summaries, we can define evidential thresholds ("How"), minimal and maximal sample sizes

For example, we can get the operational characteristics of a **fixed-n design**:

```{r analyze}
BFDA.analyze(sim.H1, design="fixed", n.max=50, boundary=6)
BFDA.analyze(sim.H0, design="fixed", n.max=50, boundary=6)
```

And for a **sequential design**:

```{r analyze2}
BFDA.analyze(sim.H1, design="sequential", n.min=20, n.max=300, boundary=10)
```
Here, all studies hit a boundary before n.max is reached. If we reduce n.max, some studies do not reach an evidential threshold:

```{r analyze3}
BFDA.analyze(sim.H1, design="sequential", n.min=20, n.max=100, boundary=10)
```

## Sample Size Determination (SSD) ##

What sample size do you need to ensure, say, 80% probability that a study design finds an effect of size, say, 0.5 with a BF >= 10?

```{r SSD1}
SSD(sim.H1, power=.80, boundary=c(1/10, 10))
```

What sample size do I need to have less than 1% of studies with a false positive error, if I set the boundary to 3?
(Note: A BF threshold of 3 is in almost all applications too lenient! Aim for a BF of at least 5).

```{r SSD2}
SSD(sim.H0, alpha=.01, boundary=c(1/3, 3))
```

Note: The SSD function automatically detects whether a H1 or a H0 simulation is analyzed.

## Including Plots

### Compare distributions of BFs for a fixed *n*

```{r compDist}
compDist(BFDA.H1=sim.H1, BFDA.H0=sim.H0, n=50, boundary=c(1/6, 6), xlim=c(1/11, 31))
```


### Open-ended sequential design

Under H1:
```{r SBF1}
plot(sim.H1, n.min=20, boundary=c(1/6, 6))
```

Under H0:
```{r SBF0}
plot(sim.H0, n.min=20, boundary=c(1/6, 6))
```


### Sequential design with n.max and asymmetric boundaries

Under H1:
```{r SBF_nmax}
plot(sim.H1, n.min=20, n.max=80, boundary=c(1/5, 10))
```
