---
title: "Bayes factor design analysis: Manual for the BFDA package"
author: "Felix Schönbrodt"
date: "`r Sys.Date()`"
---

This document demonstrates how to do a design analysis (aka. power analysis) for studies which use Bayes factors as index of evidence.
For more details about Bayes Factor Design Analysis (BFDA), see our paper:

> Schönbrodt, F. D. & Wagenmakers, E.-J. (2017). Bayes Factor Design Analysis: Planning for compelling evidence. *Psychonomic Bulletin & Review*. doi:10.3758/s13423-017-1230-y. [[PDF](https://osf.io/d4dcu)][[OSF project with reproducible code](https://osf.io/v7yxp/)]

Please note that this package is still a development version; take the results with a grain of salt.


```{r setup, include=FALSE}
	library(tint)
	knitr::opts_chunk$set(cache=TRUE, warnings = FALSE, messages=FALSE)
	# load all functions
	devtools::load_all("../package")
```

## Installation ##

The BFDA package is not on CRAN yet, but you can install the development version from Github:

```{r eval=FALSE}
library(devtools)
install_github("nicebread/BFDA", subdir="package")
```


## The general workflow ##

1. Simulate many hypothetical studies, both under H1 and under H0, using the function `BFDA.sim`
2. Analyze the simulated studies, using the function `BFDA.analyze`
3. Plot the simulated studies (`plot`, `SSD`, `evDens`)
4. Tune your design in a way that you achieve the desired goals with adequate probability (`SSD`)

To summarize, the general workflow is (here shown without parameters; these are discussed later):

```{r eval=FALSE}
sim.H1 <- BFDA.sim(expected.ES = 0.5, ...)
sim.H0 <- BFDA.sim(expected.ES = 0, ...)

BFDA.analyze(sim.H1)
BFDA.analyze(sim.H0)

plot(sim.H1)
plot(sim.H0)

SSD(sim.H1)
SSD(sim.H0)
```


## 1. Simulating hypothetical studies for a prospective design analysis

As we do not know in advance whether H1 or H0 provide a better predictive performance of real world data, we want to evaluate the performance of a design under *both* hypotheses. Hence, we have to simulate a "H1 world" and a "H0 world":

```{r sim, cache=TRUE, results='hide'}
sim.H1 <- BFDA.sim(expected.ES=0.5, type="t.between", n.min=20, n.max=300, alternative="directional", boundary=Inf, B=1000, verbose=TRUE, cores=2)

sim.H0 <- BFDA.sim(expected.ES=0, type="t.between", n.min=20, n.max=300, alternative="directional", boundary=Inf, B=1000, verbose=TRUE, cores=2)
```

Let's go through the most important parameters (for a full list of options, see `?BFDA.sim`):

- `expected.ES`: The assumed effect size (ES). In classical power analysis, this is a fixed number. Here, you can also provide a vector, which quantifies the uncertainty about the true ES. For example: `expected.ES=rnorm(100000, 0.5, 0.1)`. If a vector is provided, a new ES is drawn from this vector for each simulated study. The **metric** for `expected.ES` depends on the type of design (see net bullet point):
	- `type = "t.between` or `type = "t.paired`": expected.ES has to be provided as Cohen's *d*
	- `type = "correlation"`: expected.ES has to be provided as correlation
- `type`: Type of design. Currently, 3 designs are implemented: A between-group t-test ("t.between"), a paired t-test ("t.paired"), and correlations ("correlation")
- `n.min` and `n.max`: The initial sample size and the maximum sample size that is tested in the sequential procedure.
- `alternative`: Either "directional" for directed hypotheses (default) or "undirected" for two-sided tests
- `B`: Number of simulated studies. Aim for B >= 10,000 for stable results (in this document we use B=1000 to save some computation time).
- `cores`: Multicore support. Add as many cores as you have to speed up computations.


The simulations of the "H1 world" and a "H0 world" should have the same parameters (alternative, rscale, boundary, n.min) except the `expected.ES` (in the actual data analysis, we will apply the same test to the data set, regardless whether data came from H0 or H1 (what we don't know anyway.)).

The BFDA uses the BayesFactor package to compute between and paired t-test. By default, it uses an rscale of sqrt(2)/2 for the JZS prior on effect sizes under H1. For correlations, it uses the code provided by Wagenmakers, E. J., Verhagen, J., & Ly, A. (2016). How to quantify the evidence for the absence of a correlation. Behavior Research Methods, 1–14. http://doi.org/10.3758/s13428-015-0593-0 (see https://osf.io/cabmf/).

By default, a full sequential design without evidential stopping threshold is simulated. That means, ...
That allows to extract ...

## 2. Analyze the simulations ##

Next, we can retrieve summary statistics from our simulations. For these summaries, we can define evidential thresholds ("How"), minimal and maximal sample sizes

For example, we can get the operational characteristics of a **fixed-n design**:

```{r analyze}
BFDA.analyze(sim.H1, design="fixed", n=50, boundary=6)
BFDA.analyze(sim.H0, design="fixed", n=50, boundary=6)
```

And for a **sequential design**:

```{r analyze2}
BFDA.analyze(sim.H1, design="sequential", n.min=20, n.max=300, boundary=10)
```
Here, all studies hit a boundary before n.max is reached. If we reduce n.max, some studies do not reach an evidential threshold:

```{r analyze3}
BFDA.analyze(sim.H1, design="sequential", n.min=20, n.max=100, boundary=10)
```

## 3. Plot the design analysis

### Compare distributions of BFs for a fixed *n*

```{r evDens}
evDens(BFDA.H1=sim.H1, BFDA.H0=sim.H0, n=50, boundary=c(1/6, 6), xlim=c(1/11, 31))
```


### Open-ended sequential design

Under H1:
```{r SBF1}
plot(sim.H1, n.min=20, boundary=c(1/6, 6))
```

Under H0:
```{r SBF0}
plot(sim.H0, n.min=20, boundary=c(1/6, 6))
```


### Sequential design with n.max and asymmetric boundaries

Under H1:
```{r SBF_nmax}
plot(sim.H1, n.min=20, n.max=80, boundary=c(1/5, 10))
```

## 4. Sample Size Determination (SSD) ##

What sample size do you need to ensure, say, 80% probability that a study design finds an effect of size, say, 0.5 with a BF >= 10?

```{r SSD1}
SSD(sim.H1, power=.80, boundary=c(1/10, 10))
```

What sample size do I need to have less than 2% of studies with a false positive error, if I set the boundary to 2?
**Note: A BF threshold of 2 should never be used! Aim for a BF of at least 5. This is for didactical purposes only!**

```{r SSD2}
SSD(sim.H0, alpha=.02, boundary=c(1/2, 2))
```

Note: The SSD function automatically detects whether a H1 or a H0 simulation is analyzed. Also note that these numbers of necessary sample sizes are based on simulations and can differ between runs (see the rugged border between the shaded areas in the plot). The larger the number `B` of simulations, the less variable are these estimates.

# Paired t-test: A complete example #

The effect size metric is d_z (standardized difference scores).

```{r paired.t}
#devtools::install_github("nicebread/BFDA", subdir="package")
library(BFDA)

# do a sequential design analysis
s1 <- BFDA.sim(expected.ES=0.4, n.min=50, stepsize=5, n.max=300, type="t.paired", design="sequential", r=sqrt(2)/2, alternative="directional", cores=4)
s0 <- BFDA.sim(expected.ES=0, n.min=50, stepsize=5, n.max=300, type="t.paired", design="sequential", r=sqrt(2)/2, alternative="directional", cores=4)

# if no n.min and n.max is provided in the `BFDA.analyze` function, the values from the simulation are taken
BFDA.analyze(s1, design="sequential", boundary=10)
BFDA.analyze(s0, design="sequential", boundary=10)

BFDA.analyze(s1, design="sequential", boundary=6)
BFDA.analyze(s0, design="sequential", boundary=6)

plot(s1)
```




# Use case: Apply for a grant with sequential sampling

Granting agencies probably want a fixed sample size in the planning stage in order to quantify the amount of funding. For how much participant renumeration should one apply when using a sequential design?

We suggest to determine the requested sample size using two different design analyses:

1. Compute an open-ended SBF design with the expected sample size to get a distribution of sample sizes at stopping point.
2. Compute the 80% quantile of stopping-ns: n_q80
3. Evaluate the characteristics of a *SBF+maxN* design with n.max = n_q80. Does it have acceptable false positive and false negative error rates? What is the mean and median expected sample size?


```{r grant}
# We use the simulation from above. 
# Check the expected sample sizes for an evidential boundary of 10
a1 <- BFDA.analyze(sim.H1, design="sequential", n.min=20, boundary=10)

# --> see 80% quantile in output
a1

# Alternative approach: access stopping-ns directly
n_q80 <- ceiling(quantile(a1$endpoint.n, prob=.80))
n_q80
```

80% of all studies stop earlier than n = `r n_q80`. How does a design with that n.max perform concerning rates of misleading evidence?

```{r grant2}
a2.H1 <- BFDA.analyze(sim.H1, design="sequential", n.min=20, n.max=n_q80, boundary=10)
a2.H0 <- BFDA.analyze(sim.H0, design="sequential", n.min=20, n.max=n_q80, boundary=10)
a2.H1
a2.H0
```

In this design analysis for the *SBF+maxN* design, we can see that, although we apply for `r n_q80` participants in each group, we can expect to stop with `r a2.H1$ASN` participants or less with a 50% chance, if H1 is true. The false negative rate is virtually 0%. That means, if the effect exists in the expected size, this design virtually guarantees to detect it.

Under H0, at least half of the studies will have to use the full requested sample of `r n_q80` participants. On average, samples will have a size of n=`r a2.H0$ASN` under H0. We have a `r round(a2.H0$upper.hit.frac*100, 2)`% false positive error rate, and `r round(a2.H0$lower.hit.frac*100, 2)`% of all studies will correctly stop at the H0 boundary. The remaining `r round(a2.H0$n.max.hit.frac*100, 2)`% of all studies will remain inconclusive with respect to the desired evidential threshold of $BF_{10}$ <= 1/10. However, the Bayes factor of these studies can still be interpreted in size and direction. From the output, you can see that the majority of these inconclusive BFs still points into the correct (H0) direction.